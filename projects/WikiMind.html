<!DOCTYPE html>
<html>

  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> WikiMind | Robert Lamacraft </title>

    <!-- Favicon -->
    <link rel="icon" href="favicon.ico">
    <meta name="theme-color" content="#3CAA9C">
    <link rel="mask-icon" href="safari-pinned-tab.svg" color="#3CAA9C">

    <!-- Stylesheets -->
    <link href="../css/prism.css" rel="stylesheet">
    <link href="../css/style.css" rel="stylesheet">

    <link href='http://fonts.googleapis.com/css?family=Lato:300' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Source+Code+Pro:300' rel='stylesheet' type='text/css'>

    <script type="text/javascript" src="../js/dark-theme.js"></script>
</head>

<body>
  <section class="content-section">

    <div class="content-panel intro">
      <a href="/#homepage-content-section" class="btn header link right">More Projects</a>
      <h2> WikiMind </h2>
      <h4 class="subtitle">Can an algorithm learn a strategy that, in a reasonable amount of time, can find a chain of links between two random Wikipedia articles?</h4>
    </div>

    <div class="content-panel" id="ThePremise">
      <h3>The Premise</h3>
      <p>
        One of the best things about the Web is its self-referential meta-ness. Nowhere is that manifested better than in the glorious - and just ridiculous - games that use some of the Web’s most popular sources of information. These games include <a href="http://damn.dog/" target="_blank">damn.dog</a> (uses <a href="http://www.wikihow.com/Main-Page" target="_blank">WikiHow</a>) and <a href="https://www.geoguessr.com/" target="_blank">Geoguesser</a> (uses <a href="https://maps.google.com/" target="_blank">Google Maps</a>), but none is as brilliant and as simple as the Wikipedia game.
      </p>
      <div class="togglable-paragraphs">
        <p>
          As anyone who has found themselves down a Wikipedia-Rabbit-Hole at 2AM can attest, Wikipedia is a labyrinth of endless articles that link to more articles in all directions.
          Rather than getting lost in interesting things about European Royalty or Apollo Spacecraft, the Wikipedia game is predicated on navigating through the swamp from one random article to another - using as few links as possible - From <a href="https://en.wikipedia.org/wiki/Binary_number" target="_blank">Binary Numbers</a> to <a href="https://en.wikipedia.org/wiki/Bacon" target="_blank">Bacon</a>, that kind of thing (Binary number → Thai numerals → Thai language → Thailand → Fish sauce → Pork → Bacon).
        </p>
        <p>
          This is real test of a general awareness of a broad array of topics; after all you might get from an article about a specific WW2 battle to a particular quantum particle.
          For humans, this plays heavily on their knowledge of the content at hand; typically of history and geography.
          However, the question is, could a computer - without such a knowledge base - be able to compete simply by being able to identify what links on a page are good to follow? Is that, on its own, a viable strategy to this game?
        </p>
        <p>
          Anyone who tries navigating between articles, will quickly realise that a quick route is to follow links to broader topics: apple → vegetable, football → sport, that kind of thing.
          It is also clear that the links tend to be near the top of the page, occur frequently, perhaps in the sidebar.
        </p>
        <p>
          So, could a computer algorithm rank all of the links on a page by how useful they are in reaching broader topics, follow that link and repeat the process - all faster than it takes a human to decide how two topics might be related?
        </p>
      </div>
      <button class="togglable-paragraph-btn">Expand</button>
      <a class="btn link" href="https://github.com/rlamacraft/WikiMind/commit/9fb4b496cc19665aefcf32017704c6716b4b3890" target="_blank">CODE</a>
    </div>

    <div class="content-panel" id="RandomBlindSearch">
      <h3>Random Blind Search</h3>
      <p>
        The simplest way for a computer to play the Wikipedia game would be just a brute-force approach.
        If we think of Wikipedia as a directional graph, articles as nodes, links as edges, then we can traverse the graph with a simple searching algorithm.
        To test just how infeasible this approach is, I developed a randomised algorithm that maintained an open list of all links seen on all pages so far, iteratively picking a random link from that list, fetching its page, and adding all of the links on that page to the open list.
        But this quickly became unmanageable.
      </p>
      <div class="togglable-paragraphs">
        <p>
          And that’s the problem.
          Most Wikipedia articles have a branching factor in the 10s or even 100s.
          Running for just a few minutes, and the open list grows to tens or even hundreds of thousands of links, most of which are complete useless.
          And neither breadth-first nor depth-first search would fare any better in trying to find a path from two purely random articles.
          We need some heuristics!
        </p>
        <p>
          So the question that is raised, is how do we improve this algorithm so that it can at least find <b>a</b> path between two articles.
          As soon as we find a single path we can run the algorithm on many, many pairs of random articles, iteratively improving the algorithm but first we need something that can find an initial path.
          What heuristics might be able to make the searching actual achievable?
        </p>
      </div>
      <button class="togglable-paragraph-btn">Expand</button>
      <a class="btn link" href="https://github.com/rlamacraft/WikiMind/commit/26ddc828d92d938d5df2e24295eccc2c9b6fe8cc" target="_blank">CODE</a>
    </div>

    <div class="content-panel" id="WhyMachineLearning">
      <h3>Why Machine Learning?</h3>
      <p>
        Let’s take a step back for a moment.
        Machine learning is an up and coming field of computing for producing domain-independent algorithms that,
        given a large set of data, can find patterns in the data and then be able to predict the value of a data point not previously seen before.
        A common type is a classifier that given lots of data can learn to classify, well, things. This <a href="https://www.youtube.com/playlist?list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal" target="_blank">YouTube series</a> by Google gives a good introduction.
        Machine learning is a good fit for this project because we don’t know what the optimal strategy is to navigate through Wikipedia.
      </p>
      <div class="togglable-paragraphs">
        <p>
          Intuitively, links at the top of the page, for example, would be a good indicator that they are good links to follow.
          They are likely to be links to broader topics, and as previously mentioned, are likely to help us to complete the chain.
          However, this might not also be the case (turns out links that appear at the very end of the document also appear to be pretty good).
          Furthermore, other features may be less obvious: is it good if a link appears multiple times on a page? What about if we have seen the link on another page?
          We can simply collect all of this data once we have a very basic algorithm, feed it into a machine learning algorithm that can then learn to classify links.
          We can then use the links the algorithm classifies highly to find more chains, and repeat the process of gathering data, learning, and searching.
        </p>
        <p>
          After some thought, the initial features used for collecting this data will be the distance through the HTML of the document the link first appears
          (as a percentage of the document to accommodate for the fact the articles vary wildly in length) and the number of times the text enclosed by the link appears on the page.
          These are both intuitive and clear features that hopefully will be able to produce understandable results (hopefully).
          Later, more complex features can be added that may have unexpected results.
          The measure of success, for determining whether a link is good or bad, will be the length of the chain; so shorter chains contain good links.
          An imperfect heuristic, but it'll do for now.
        </p>
      </div>
      <button class="togglable-paragraph-btn">Expand</button>
      <a class="btn link" href="https://github.com/rlamacraft/WikiMind/commit/8923e113105829b0d65088b3963b3c28c9f69280" target="_blank">CODE</a>
    </div>

    <div class="content-panel" id="SearchingForAnInitialDataSet">
      <h3>Searching for an Initial Data Set</h3>
      <p>
        As random searching proved, before we can just have a go at a search between two random articles we need some intelligent algorithm for reducing the number of pages we have to search.
        We can use a machine learning classifier to do this, to rank links into a number of different buckets based on their perceived usefulness in finding a chain.
        However, in order to train a classifier we need an initial data set. It doesn't have to be perfect (we can improve it iteratively over time) but we need something to start with.
      </p>
      <div class="togglable-paragraphs">
        <p>
          We can do this by simplifying the problem.
          Instead of searching for a chain between two random articles, we can search from a random article - let's call it article A(0) - to another article that is derived from article A(0).
          Let's call the derived article A(N) where N is the number of link we follow. So, to generate A(2) we start at a random article A(0), follow a link to a page A(1), then follow a link in A(1) to get to an article A(2).
          These links can be selected initially at random but then increasingly using the classifier as it learns.
          We can also increment N as the classifier becomes more capable, and by starting at a small value of N (say, less than 3) we can find chains using nothing but random searching to bootstrap the whole process.
        </p>
        <p>
          So, how does it work?
          The main script, first_searcher.py, runs a loop that creates a search, running for some number of iterations (a few dozen times gets a good sample of data).
          The search function generates a random article, A(0), using Wikipedia's API, and then finds a goal, A(N), derived from A(0), the classifier, and some random factor.
          The searcher then initialises an open list (a list of links to articles that have been discovered but not yet visited) with A(0), and continues to loop until either the open list is empty or a chain from A(0) to A(N) has been found.
          In batches of up to 10, links are pulled from the open list, fetched, analysed for links to the A(N) article, and all of their links are added to the open list - using the classifier to calculate their scores in the process.
          The open list is maintained as a heap to give easy access to the links with the best scores, but the batches are also populated with a small number of random links from the open list to ensure the algorithm is continually trying new approaches and will eventually learn from those approaches that work.
        </p>
        <p>
          Then, once we have a chain between A(0) and A(N), we add all of the links in the chain to the results data structure with their scores, as well as a selection of the links that the algorithm fetched but failed to be useful (recorded with a score of 10, greater than any successful score, to penalise those data points).
          The proportion of these useless requests that are recorded is determined by the function: <code>round( log( len( useless_requests ), 10) ** 4)</code>.
          This means that the number of recorded useless requests is proportional to the number of useless requests made, but doesn't grow quite as quickly.
          This is important as without any checks the searching algorithm makes A LOT of requests.
          Currently, a cap of 50,000 links on the open list causes the algorithm to just give up but this isn't ideal: firstly because 50,000 is very arbitrary, it is impossible to tell if the next link would have been the one to complete the chain, and the fixed limit does not increase as N (the depth of the constructed chain) grows.
        </p>
        <p>
          That's all well and good, but does it work? Kind of.
          For values of N less than about 8, the algorithm finds chains most of the time - often much shorter chains than the initial chain - but has to give up a good portion of the time.
          For values greater than 8, the initial chain too often either gets stuck in a loop - visiting the same page twice in the initial chain, and so in effect producing a chain of size 3 or 4 - or falling down a 'well' that is impossible to find.
          These 'well's are links that point to a very different part of Wikipedia (the article on <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" target="_blank" aria-label="Wikipedia: ISBN">ISBNs</a> is a frequent example of a well as it occurs in the references of lots of pages) and so any subsequent links are very difficult to find.
          And this highlights the biggest flaw with this system - it relies heavily on a highly connected graph of linked articles and although it discovers broader topics it struggles to find the correct topic to link wider articles together.
          What is needed is a two-sided approach so that rather than searching for a needle in a haystack, we're searching for intersection of two haystacks, but first more data is needed.
          The two currently recorded features, the frequency that the text of a link occurs on a page and the percentage of the length through the HTML that the link first appears, are useful but lacking.
          The data is murky and fails to identify obvious connections. Additional, richer, features of links will help identify these deficiencies and improve the classifier before we let it loose on the real problem.
        </p>
      </div>
      <button class="togglable-paragraph-btn">Expand</button>
      <a class="btn link" href="https://github.com/rlamacraft/WikiMind/commit/042f8172233530b598e011fa18a3773e4d9fe0bc" target="_blank">CODE</a>
    </div>

    <div class="content-panel outro">
      <a href="/#homepage-content-section" class="btn header link right">More Projects</a>
    </div>

  </section>

  <script
    src="https://code.jquery.com/jquery-3.2.1.js"
    integrity="sha256-DZAnKJ/6XZ9si04Hgrsxu/8s717jcIzLy3oi35EouyE="
    crossorigin="anonymous">
  </script>
  <script type="text/javascript" src="../js/prism.js"></script>
  <script>
    $(".togglable-paragraphs").slideUp(0);
    if(window.location.hash !== "") {
      $(window.location.hash).children(".togglable-paragraphs").slideDown().toggleClass('open');
      $(window.location.hash).children(".togglable-paragraph-btn").text("Collapse");
    }
    $(".togglable-paragraph-btn").click(function(evt) {
      var button = $(evt.target);
      var paragraph = $(button.prev()[0]);
      if((paragraph).hasClass("open")) {
        paragraph.slideUp();
        button.text("Expand");
      } else {
        paragraph.slideDown();
        button.text("Collapse");
      }
      paragraph.toggleClass("open");
    });
  </script>

</body>
</html>
